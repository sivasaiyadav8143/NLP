# ğŸ“˜ Frequency-Based Word Embeddings in NLP

This repository contains hands-on implementations of foundational word embedding techniques â€” Bag-of-Words, N-grams, and TF-IDF â€” as introduced in the article [Word Embeddings in NLP: From Bag-of-Words to Transformers (Part 1)](https://medium.com/p/4688627a728f).

These frequency-based methods are the building blocks of modern NLP pipelines. They help transform raw text into numerical representations that can be used for classification, search, and clustering.

---

## ğŸ“‚ Repository Structure

```
frequency_embeddings/
â”œâ”€â”€ Bag-of-Words/
â”‚   â”œâ”€â”€ bow_support_ticket_classifier.ipynb
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ N-grams/
â”‚   â”œâ”€â”€ ngrams_hotel_review.ipynb
â”‚   â”œâ”€â”€ ngrams_hotel_review.md
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ TF-IDF/
    â”œâ”€â”€ tfidf_doc_search.ipynb
    â””â”€â”€ requirements.txt
```
---

## ğŸ” Modules Overview

### ğŸ§¾ Bag-of-Words
- **Use case**: Classify support tickets based on keyword frequency.
- **Highlights**: Sparse vector representation, vocabulary size control, basic preprocessing.

### ğŸ§¾ N-grams
- **Use case**: Analyze hotel reviews using bigrams and trigrams.
- **Highlights**: Phrase-level context, token sequencing, n-gram generation.

### ğŸ§¾ TF-IDF
- **Use case**: Document search based on term importance.
- **Highlights**: Term weighting, inverse document frequency, cosine similarity.

---

## ğŸ› ï¸ Setup

```bash
# Create environment
python -m venv freq_embed_env
source freq_embed_env/bin/activate

# Install dependencies
pip install -r requirements.txt  # Run inside each module folder
```

ğŸ“š Related Reading
This repo complements the concepts discussed in:

ğŸ“„ Medium Article  
Word Embeddings in NLP: From Bag-of-Words to Transformers (Part 1) [Medium](https://medium.com/@sivasai-yadav)
Explore how frequency-based methods laid the groundwork for semantic embeddings like Word2Vec, GloVe, and BERT.

ğŸ¤ Contributions
Feel free to fork, improve, or extend the notebooks with your own datasets or embedding techniques. Pull requests welcome!
