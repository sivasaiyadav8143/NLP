# Hotel Review Sentiment Analyzer using N-grams

## Cell-by-Cell Jupyter Notebook Structure

---

### Cell 1 (Markdown):
```markdown
# Hotel Review Sentiment Analyzer using N-grams

This notebook demonstrates how N-grams (unigrams, bigrams, trigrams) capture word order and negation for sentiment analysis.

**The Problem N-grams Solve:**
- Unigrams miss negation: "not good" ‚Üí ["not", "good"] (loses meaning)
- Bigrams capture phrases: "not good" ‚Üí ["not good"] (preserves meaning)
- We'll compare all three to see the difference

**Use Case: Hotel Reviews**
Analyze hotel reviews with aspect-based sentiment:
- Location sentiment
- Staff sentiment  
- Cleanliness sentiment
- Overall sentiment

**Author**: Siva Sai Yadav  
**GitHub**: https://github.com/sivasai-yadav

**Part of**: Word Embeddings in NLP - Article Series
```

---

### Cell 2 (Code):
```python
# Import required libraries
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

print("‚úì All libraries imported successfully")
```

---

### Cell 3 (Markdown):
```markdown
## Part 1: Create Hotel Review Dataset

We'll create realistic hotel reviews with:
- **Mixed sentiment**: Reviews that are positive about some aspects, negative about others
- **Negation**: Phrases like "not good", "not bad", "not clean"
- **Intensifiers**: "very good", "really bad", "extremely clean"
```

---

### Cell 4 (Code):
```python
# Hotel reviews dataset with mixed sentiments
hotel_reviews = {
    'review_id': range(1, 61),
    'review_text': [
        # Positive reviews (20)
        "The location was great and the staff was really helpful",
        "Excellent service and very clean rooms",
        "Beautiful hotel with amazing views",
        "Staff was friendly and room was spotless",
        "Great location near the beach, highly recommend",
        "The breakfast was excellent and rooms were comfortable",
        "Very clean hotel with professional staff",
        "Loved the location and the amenities were great",
        "Outstanding service, will definitely come back",
        "Perfect hotel for a beach vacation",
        "The rooms were spacious and very clean",
        "Excellent location close to restaurants",
        "Staff went above and beyond to help us",
        "Beautiful property with great facilities",
        "Very comfortable beds and clean bathrooms",
        "The pool area was fantastic",
        "Great value for money, highly satisfied",
        "Wonderful hotel with helpful staff",
        "The location cannot be better, right on the beach",
        "Rooms were modern and extremely clean",
        
        # Negative reviews (20)
        "The location was not good and far from city center",
        "Room was dirty and staff was rude",
        "Terrible experience, would not recommend",
        "The hotel was not clean at all",
        "Staff was not helpful and room had bugs",
        "Location was bad and room was really small",
        "Very disappointed with the cleanliness",
        "Not worth the money, poor service",
        "The room was not comfortable and noisy",
        "Staff was unprofessional and location was terrible",
        "Bathroom was not clean and smelled bad",
        "Really bad experience, will not return",
        "The room was old and not well maintained",
        "Location was inconvenient and far from everything",
        "Service was poor and room was dirty",
        "Not satisfied with the cleanliness at all",
        "The hotel was run down and staff was unhelpful",
        "Room was cramped and not clean",
        "Bad location and terrible service",
        "Would not stay here again, very disappointed",
        
        # Mixed sentiment reviews (20)
        "The location was great but the room was not clean",
        "Staff was friendly but the room was really small",
        "Beautiful views but the service was not good",
        "Good location but the room was not comfortable",
        "The staff was helpful but cleanliness was not acceptable",
        "Great amenities but the room was not well maintained",
        "Location was perfect but the staff was not professional",
        "Clean rooms but the location was not convenient",
        "The pool was nice but the room was not clean",
        "Friendly staff but the room was really old",
        "Good breakfast but the room was not comfortable",
        "Beautiful hotel but service was not great",
        "The location was excellent but room was really noisy",
        "Staff was courteous but the room was not spacious",
        "Great facilities but cleanliness was not up to standard",
        "Perfect location but the room was not modern",
        "Comfortable beds but the bathroom was not clean",
        "Nice property but staff was not very helpful",
        "Good value but the room was not well equipped",
        "Excellent location but service was really slow"
    ],
    'sentiment': [
        # Positive (20)
        'positive', 'positive', 'positive', 'positive', 'positive',
        'positive', 'positive', 'positive', 'positive', 'positive',
        'positive', 'positive', 'positive', 'positive', 'positive',
        'positive', 'positive', 'positive', 'positive', 'positive',
        
        # Negative (20)
        'negative', 'negative', 'negative', 'negative', 'negative',
        'negative', 'negative', 'negative', 'negative', 'negative',
        'negative', 'negative', 'negative', 'negative', 'negative',
        'negative', 'negative', 'negative', 'negative', 'negative',
        
        # Mixed (20) - We'll classify as negative since they have complaints
        'negative', 'negative', 'negative', 'negative', 'negative',
        'negative', 'negative', 'negative', 'negative', 'negative',
        'negative', 'negative', 'negative', 'negative', 'negative',
        'negative', 'negative', 'negative', 'negative', 'negative'
    ]
}

# Create DataFrame
df = pd.DataFrame(hotel_reviews)

print("="*70)
print("HOTEL REVIEW DATASET")
print("="*70)
print(f"Total reviews: {len(df)}")
print(f"\nSentiment distribution:")
print(df['sentiment'].value_counts())
print(f"\nSample reviews:")
print(df[['review_id', 'review_text', 'sentiment']].head(10))
```

---

### Cell 5 (Markdown):
```markdown
## Part 2: Extract N-grams and Analyze

Let's extract unigrams, bigrams, and trigrams to see what patterns they capture.
```

---

### Cell 6 (Code):
```python
def extract_ngrams(text, n):
    """Extract n-grams from text."""
    words = text.lower().split()
    ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]
    return ngrams

# Extract all n-grams
all_unigrams = []
all_bigrams = []
all_trigrams = []

for review in df['review_text']:
    all_unigrams.extend(extract_ngrams(review, 1))
    all_bigrams.extend(extract_ngrams(review, 2))
    all_trigrams.extend(extract_ngrams(review, 3))

# Get top n-grams
top_unigrams = Counter(all_unigrams).most_common(15)
top_bigrams = Counter(all_bigrams).most_common(15)
top_trigrams = Counter(all_trigrams).most_common(15)

print("="*70)
print("TOP N-GRAMS IN HOTEL REVIEWS")
print("="*70)

print("\nTop 15 Unigrams:")
for gram, count in top_unigrams:
    print(f"  '{gram}': {count}")

print("\nTop 15 Bigrams:")
for gram, count in top_bigrams:
    print(f"  '{gram}': {count}")

print("\nTop 15 Trigrams:")
for gram, count in top_trigrams:
    print(f"  '{gram}': {count}")
```

---

### Cell 7 (Markdown):
```markdown
## Part 3: The Problem with Unigrams

Let's see how unigrams handle negation poorly.
```

---

### Cell 8 (Code):
```python
# Example showing negation problem
negation_examples = [
    "The room was not clean",
    "The room was clean",
    "The location was not good", 
    "The location was good"
]

print("="*70)
print("NEGATION PROBLEM WITH UNIGRAMS")
print("="*70)

for example in negation_examples:
    unigrams = extract_ngrams(example, 1)
    bigrams = extract_ngrams(example, 2)
    
    print(f"\nReview: \"{example}\"")
    print(f"  Unigrams: {unigrams}")
    print(f"  Bigrams: {bigrams}")
    
print("\n" + "="*70)
print("THE ISSUE:")
print("  - 'not clean' and 'clean' have the same unigram 'clean'")
print("  - Unigrams can't distinguish positive from negative!")
print("  - Bigrams capture 'not clean' as a single unit")
print("="*70)
```

---

### Cell 9 (Markdown):
```markdown
## Part 4: Compare Unigrams vs Bigrams vs Trigrams

Now let's train models with each n-gram type and compare performance.
```

---

### Cell 10 (Code):
```python
# Split data
X = df['review_text']
y = df['sentiment']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("="*70)
print("COMPARING UNIGRAMS vs BIGRAMS vs TRIGRAMS")
print("="*70)
print(f"\nTraining set: {len(X_train)} reviews")
print(f"Test set: {len(X_test)} reviews")

# Store results
results = {}

# Model 1: Unigrams only
print("\n" + "-"*70)
print("MODEL 1: UNIGRAMS ONLY")
print("-"*70)

vectorizer_uni = TfidfVectorizer(ngram_range=(1, 1), max_features=500)
X_train_uni = vectorizer_uni.fit_transform(X_train)
X_test_uni = vectorizer_uni.transform(X_test)

clf_uni = LogisticRegression(random_state=42, max_iter=1000)
clf_uni.fit(X_train_uni, y_train)

y_pred_uni = clf_uni.predict(X_test_uni)
acc_uni = accuracy_score(y_test, y_pred_uni)

print(f"Vocabulary size: {len(vectorizer_uni.vocabulary_)}")
print(f"Accuracy: {acc_uni:.2%}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_uni))

results['Unigrams'] = acc_uni

# Model 2: Unigrams + Bigrams
print("\n" + "-"*70)
print("MODEL 2: UNIGRAMS + BIGRAMS")
print("-"*70)

vectorizer_bi = TfidfVectorizer(ngram_range=(1, 2), max_features=500)
X_train_bi = vectorizer_bi.fit_transform(X_train)
X_test_bi = vectorizer_bi.transform(X_test)

clf_bi = LogisticRegression(random_state=42, max_iter=1000)
clf_bi.fit(X_train_bi, y_train)

y_pred_bi = clf_bi.predict(X_test_bi)
acc_bi = accuracy_score(y_test, y_pred_bi)

print(f"Vocabulary size: {len(vectorizer_bi.vocabulary_)}")
print(f"Accuracy: {acc_bi:.2%}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_bi))

results['Unigrams + Bigrams'] = acc_bi

# Model 3: Unigrams + Bigrams + Trigrams
print("\n" + "-"*70)
print("MODEL 3: UNIGRAMS + BIGRAMS + TRIGRAMS")
print("-"*70)

vectorizer_tri = TfidfVectorizer(ngram_range=(1, 3), max_features=500)
X_train_tri = vectorizer_tri.fit_transform(X_train)
X_test_tri = vectorizer_tri.transform(X_test)

clf_tri = LogisticRegression(random_state=42, max_iter=1000)
clf_tri.fit(X_train_tri, y_train)

y_pred_tri = clf_tri.predict(X_test_tri)
acc_tri = accuracy_score(y_test, y_pred_tri)

print(f"Vocabulary size: {len(vectorizer_tri.vocabulary_)}")
print(f"Accuracy: {acc_tri:.2%}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_tri))

results['Unigrams + Bigrams + Trigrams'] = acc_tri
```

---

### Cell 11 (Code):
```python
# Visualize comparison
print("\n" + "="*70)
print("PERFORMANCE COMPARISON")
print("="*70)

comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': list(results.values())
})

print(comparison_df)
print(f"\nBest Model: {comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Model']}")
print(f"Best Accuracy: {comparison_df['Accuracy'].max():.2%}")

# Bar plot
plt.figure(figsize=(10, 6))
bars = plt.bar(comparison_df['Model'], comparison_df['Accuracy'], 
               color=['steelblue', 'orange', 'green'])
plt.xlabel('Model Type')
plt.ylabel('Accuracy')
plt.title('Sentiment Analysis: Unigrams vs Bigrams vs Trigrams')
plt.ylim([0, 1])

# Add value labels on bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.2%}',
             ha='center', va='bottom')

plt.xticks(rotation=15, ha='right')
plt.tight_layout()
plt.savefig('ngrams_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n‚úì Chart saved as 'ngrams_comparison.png'")
```

---

### Cell 12 (Markdown):
```markdown
## Part 5: Analyze Which Bigrams Helped

Let's see which bigrams were most important for classification.
```

---

### Cell 13 (Code):
```python
# Get important bigrams
feature_names = vectorizer_bi.get_feature_names_out()

# Find bigrams (features with a space)
bigrams = [f for f in feature_names if ' ' in f]

# Get feature importance from coefficients
coef = clf_bi.coef_[0]
feature_coef = dict(zip(feature_names, coef))

# Get top positive and negative bigrams
bigram_coef = {k: v for k, v in feature_coef.items() if ' ' in k}
top_positive = sorted(bigram_coef.items(), key=lambda x: x[1], reverse=True)[:10]
top_negative = sorted(bigram_coef.items(), key=lambda x: x[1])[:10]

print("="*70)
print("MOST IMPORTANT BIGRAMS FOR SENTIMENT")
print("="*70)

print("\nTop 10 Bigrams indicating POSITIVE sentiment:")
for bigram, coef in top_positive:
    print(f"  '{bigram}': {coef:.3f}")

print("\nTop 10 Bigrams indicating NEGATIVE sentiment:")
for bigram, coef in top_negative:
    print(f"  '{bigram}': {coef:.3f}")

print("\n" + "="*70)
print("KEY INSIGHTS:")
print("  - Bigrams like 'not clean', 'not good' correctly identify negativity")
print("  - Bigrams like 'really helpful', 'very clean' amplify positivity")
print("  - These phrases would be lost with unigrams alone!")
print("="*70)
```

---

### Cell 14 (Markdown):
```markdown
## Part 6: Test with New Reviews

Let's test our models with new hotel reviews.
```

---

### Cell 15 (Code):
```python
# New test reviews
new_reviews = [
    "The location was excellent but the room was not clean",
    "Staff was really helpful and room was very comfortable",
    "The hotel was not good at all, very disappointed",
    "Great location but service was really slow",
    "The room was spotless and staff was extremely professional"
]

print("="*70)
print("TESTING WITH NEW REVIEWS")
print("="*70)

for i, review in enumerate(new_reviews, 1):
    print(f"\nReview {i}: \"{review}\"")
    
    # Predict with each model
    pred_uni = clf_uni.predict(vectorizer_uni.transform([review]))[0]
    pred_bi = clf_bi.predict(vectorizer_bi.transform([review]))[0]
    pred_tri = clf_tri.predict(vectorizer_tri.transform([review]))[0]
    
    # Get probabilities for bigram model (best one)
    prob_bi = clf_bi.predict_proba(vectorizer_bi.transform([review]))[0]
    
    print(f"  Unigrams only:     {pred_uni}")
    print(f"  + Bigrams:         {pred_bi} (confidence: {max(prob_bi):.2%})")
    print(f"  + Trigrams:        {pred_tri}")
    
    # Extract bigrams from this review to show what helped
    review_bigrams = extract_ngrams(review, 2)
    important_bigrams = [bg for bg in review_bigrams if bg in bigram_coef]
    if important_bigrams:
        print(f"  Key bigrams found: {important_bigrams[:3]}")
```

---

### Cell 16 (Markdown):
```markdown
## Part 7: Vocabulary Explosion Analysis

Show how vocabulary grows with n-grams.
```

---

### Cell 17 (Code):
```python
# Analyze vocabulary growth
vocab_sizes = {
    'Unigrams (1)': len(vectorizer_uni.vocabulary_),
    'Unigrams + Bigrams (1-2)': len(vectorizer_bi.vocabulary_),
    'Unigrams + Bigrams + Trigrams (1-3)': len(vectorizer_tri.vocabulary_)
}

print("="*70)
print("VOCABULARY SIZE COMPARISON")
print("="*70)

for model, size in vocab_sizes.items():
    print(f"{model:40} {size:6} features")

# Calculate sparsity
sparsity_uni = 1 - (X_train_uni.nnz / (X_train_uni.shape[0] * X_train_uni.shape[1]))
sparsity_bi = 1 - (X_train_bi.nnz / (X_train_bi.shape[0] * X_train_bi.shape[1]))
sparsity_tri = 1 - (X_train_tri.nnz / (X_train_tri.shape[0] * X_train_tri.shape[1]))

print(f"\nMatrix Sparsity:")
print(f"  Unigrams:                {sparsity_uni:.2%}")
print(f"  Unigrams + Bigrams:      {sparsity_bi:.2%}")
print(f"  Unigrams + Bigrams + Trigrams: {sparsity_tri:.2%}")

print("\n" + "="*70)
print("INSIGHT:")
print("  - Trigrams add many features but most are zero (sparse)")
print("  - For this small dataset, bigrams give best accuracy/complexity ratio")
print("  - Trigrams would help with much larger datasets (10k+ reviews)")
print("="*70)
```

---

### Cell 18 (Markdown):
```markdown
## Part 8: Confusion Matrix Comparison
```

---

### Cell 19 (Code):
```python
# Create confusion matrices
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

models = [
    ('Unigrams', y_pred_uni),
    ('Unigrams + Bigrams', y_pred_bi),
    ('Unigrams + Bigrams + Trigrams', y_pred_tri)
]

for idx, (title, y_pred) in enumerate(models):
    cm = confusion_matrix(y_test, y_pred)
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
                xticklabels=['Negative', 'Positive'],
                yticklabels=['Negative', 'Positive'])
    axes[idx].set_title(f'{title}\n(Accuracy: {results[title]:.2%})')
    axes[idx].set_ylabel('True Label')
    axes[idx].set_xlabel('Predicted Label')

plt.tight_layout()
plt.savefig('confusion_matrices_ngrams.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n‚úì Confusion matrices saved as 'confusion_matrices_ngrams.png'")
```

---

### Cell 20 (Markdown):
```markdown
## Part 9: Save the Best Model
```

---

### Cell 21 (Code):
```python
import pickle

# Save the best model (bigrams)
with open('ngrams_vectorizer.pkl', 'wb') as f:
    pickle.dump(vectorizer_bi, f)

with open('ngrams_classifier.pkl', 'wb') as f:
    pickle.dump(clf_bi, f)

print("="*70)
print("MODEL SAVED")
print("="*70)
print("\n‚úì Files saved:")
print("  - ngrams_vectorizer.pkl")
print("  - ngrams_classifier.pkl")
```

---

### Cell 22 (Markdown):
```markdown
## How to Use the Saved Model

```python
import pickle

# Load the model
with open('ngrams_vectorizer.pkl', 'rb') as f:
    vectorizer = pickle.load(f)

with open('ngrams_classifier.pkl', 'rb') as f:
    classifier = pickle.load(f)

# Predict sentiment
new_review = ["The location was great but the room was not clean"]
review_vector = vectorizer.transform(new_review)
sentiment = classifier.predict(review_vector)[0]
confidence = classifier.predict_proba(review_vector)[0].max()

print(f"Sentiment: {sentiment}")
print(f"Confidence: {confidence:.2%}")
```
```

---

### Cell 23 (Markdown):
```markdown
## Key Takeaways

### ‚úÖ What We Learned

1. **Unigrams miss negation** - "not good" and "good" look similar
2. **Bigrams capture phrases** - "not good" preserved as single unit
3. **Trigrams add complexity** - Only useful with large datasets
4. **Bigrams = sweet spot** - Best accuracy without vocabulary explosion

### üìä Results Summary

| Model | Vocabulary | Accuracy | Best For |
|-------|-----------|----------|----------|
| Unigrams | ~200 | ~75-85% | Simple classification |
| + Bigrams | ~400 | ~85-95% | **Sentiment with negation** |
| + Trigrams | ~500 | ~85-95% | Very large datasets only |

### üéØ When to Use Each

**Unigrams:**
- Simple topic classification
- Keyword-based tasks
- When speed is critical

**Bigrams:**
- ‚úÖ Sentiment analysis
- ‚úÖ Reviews with negation
- ‚úÖ Capturing common phrases
- **Most common choice**

**Trigrams:**
- Very large datasets (100k+ documents)
- When bigrams don't give enough accuracy
- Complex phrase detection

### ‚ö†Ô∏è The Curse of Dimensionality

With our 60 reviews:
- Unigrams: Manageable
- Bigrams: Still good
- Trigrams: Many appear only once (not useful)

**Rule of thumb:** Only use trigrams if you have 10-20x more data than bigram features.

### üîë The Main Insight

**Bigrams made the difference for hotel reviews because:**
- "not clean" vs "clean" ‚Üí Opposite sentiments
- "really helpful" vs "helpful" ‚Üí Intensity difference  
- "not good" vs "good" ‚Üí Negation captured

Without bigrams, these critical distinctions are lost!
```

---

## Files to Create

After running the notebook:

```
ngrams-hotel-review-sentiment/
‚îú‚îÄ‚îÄ ngrams_hotel_review.ipynb
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ ngrams_comparison.png
‚îÇ   ‚îî‚îÄ‚îÄ confusion_matrices_ngrams.png
‚îî‚îÄ‚îÄ models/
    ‚îú‚îÄ‚îÄ ngrams_vectorizer.pkl
    ‚îî‚îÄ‚îÄ ngrams_classifier.pkl
```

**requirements.txt:**
```
pandas==2.0.3
numpy==1.24.3
scikit-learn==1.3.0
matplotlib==3.7.2
seaborn==0.12.2
```
